services:
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    hostname: namenode
    environment:
      - CLUSTER_NAME=hadoop-cluster
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - HDFS_CONF_dfs_namenode_name_dir=file:///hadoop/dfs/name
    ports:
      - "9870:9870"   # Web UI du NameNode
      - "9000:9000"   # Port RPC HDFS utilisÃ© par fs.defaultFS
    volumes:
      - namenode:/hadoop/dfs/name
      - ./data-local:/data-local
    networks:
      - hadoop

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    hostname: datanode
    depends_on:
      - namenode
    environment:
      - CLUSTER_NAME=hadoop-cluster
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - HDFS_CONF_dfs_datanode_data_dir=file:///hadoop/dfs/data
      #- HDFS_CONF_dfs_datanode_hostname=datanode #important pour pouvoir deposer le fichier => Ã  commenter aprÃ¨s pour activer la ligne suivante
      - HDFS_CONF_dfs_datanode_hostname=localhost  # utile pour pouvoir lire depuis une application externe au docker-compose sans devoir modifier le fichier /et/hosts "127.0.0.1 datanode"
    ports:
      - "9866:9866"   # ðŸ”¥ Port DATA pour les blocs HDFS
      - "9864:9864"   # Web UI du DataNode (optionnel)
    volumes:
      - datanode:/hadoop/dfs/data
      - ./data-local:/data-local
    networks:
      - hadoop

  #spring-app:
  #  build:
  #    context: ./../spark-read-hdfs-file
  #    dockerfile: Dockerfile
  #  container_name: spring-app
  #  depends_on:
  #    - namenode
  #  networks:
  #    - hadoop
  #  environment:
  #    - SPRING_PROFILES_ACTIVE=default
  #  ports:
  #    - "8080:8080"

volumes:
  namenode:
  datanode:

networks:
  hadoop: